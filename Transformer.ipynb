{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HGzTdFJQII1q"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math,copy,time\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "%matplotlib inline\n",
    "\n",
    "#from google.colab import files\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3AEhrPSxIIfF"
   },
   "source": [
    "# **Helper Class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74,
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "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",
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": ""
      }
     }
    },
    "colab_type": "code",
    "id": "SNrsb6tH3Rte",
    "outputId": "14bdc1f1-bc0d-4a5b-99f9-609637177e7e"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6VJIDDK_IOZy"
   },
   "outputs": [],
   "source": [
    "class Lang:\n",
    "    \"\"\"\n",
    "    Helper class for creating dictionaries for languages\n",
    "    \"\"\"\n",
    "    def __init__(self,name):\n",
    "        self.name = name\n",
    "        self.word2index = {'SOS':0,'EOS':1,'PAD':2}\n",
    "        self.index2word = {0:'SOS',1:'EOS',2:'PAD'}\n",
    "        self.word2count = {}\n",
    "        self.n_words = 3\n",
    "\n",
    "    def add_sentence(self,sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.add_word(word)\n",
    "\n",
    "            \n",
    "    def add_word(self,word):\n",
    "        # check if the word has appeared before\n",
    "        if word not in self.word2index:\n",
    "            # word to index at index (n_words)\n",
    "            self.word2index[word] = self.n_words\n",
    "            \n",
    "            # index (n_words) to word\n",
    "            self.index2word[self.n_words] = word\n",
    "            \n",
    "            # word count is set to 1\n",
    "            self.word2count[word] = 1\n",
    "            \n",
    "            # numbers of unique words increases by 1\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9IqhQwomITXJ"
   },
   "outputs": [],
   "source": [
    "def read_Lang(reverse=False):\n",
    "    # open the file in the directory\n",
    "    # This is the only class that is language specific\n",
    "    \n",
    "    # Reads chinese to english files, the first line is English and second line is Chinese\n",
    "    f = open(\"cmn-eng.txt\",encoding='utf-8')\n",
    "    \n",
    "    # read lines in the file\n",
    "    lines = f.readlines()\n",
    "    lines = [line.rstrip().split('\\t')[0:2] for line in lines]\n",
    "    \n",
    "    # add all the lower case alphabetical letters into Chinese dictionary\n",
    "    alph = string.ascii_lowercase\n",
    "    alph = [s for s in alph]\n",
    "    alph = \" \".join(alph)\n",
    "\n",
    "    # close the file\n",
    "    f.close()\n",
    "    \n",
    "    if reverse:\n",
    "        lines = [list(reversed(line)) for line in lines]\n",
    "        \n",
    "        # Input is Chinese\n",
    "        input_ = [re.sub(r\" \",\"\",line[0]) for line in lines]\n",
    "        input_ = [[word for word in line] for line in input_]\n",
    "        input_ = [\" \".join(line) for line in input_]\n",
    "        input_ = [line.lower() for line in input_]\n",
    "        input_.append(alph)\n",
    "        \n",
    "        # Output is English\n",
    "        output_ = [re.sub(r\"([.!?])\",r\" \\1\",line[1]) for line in lines]\n",
    "        output_ =  [re.sub(r\"[^a-zA-Z.!?]+\", r\" \", line) for line in output_]\n",
    "        output_ = [s.lower() for s in output_]\n",
    "        \n",
    "        # Combine the pairs\n",
    "        pairs = [[i,o] for i,o in zip(input_,output_)]\n",
    "        input_class = Lang('cmn')\n",
    "        output_class = Lang('eng')\n",
    "    else:\n",
    "        # input is English\n",
    "        input_ = [re.sub(r\"([.!?])\",r\" \\1\",line[0]) for line in lines]\n",
    "        input_ =  [re.sub(r\"[^a-zA-Z.!?]+\", r\" \", line) for line in input_]\n",
    "        input_ = [s.lower() for s in input_]\n",
    "        \n",
    "        # output is Chinese\n",
    "        output_ = [re.sub(r\" \",\"\",line[1]) for line in lines]\n",
    "        output_ = [[word for word in line] for line in output_]\n",
    "        output_ = [\" \".join(line) for line in output_]\n",
    "        output_ = [line.lower() for line in output_]\n",
    "        output_.append(alph)\n",
    "        \n",
    "        # Combine the pairs\n",
    "        pairs = [[i,o] for i,o in zip(input_,output_)]\n",
    "        input_class = Lang('eng')\n",
    "        output_class = Lang('cmn')\n",
    "    \n",
    "    \n",
    "    for i in range(len(input_)):\n",
    "        input_class.add_sentence(input_[i])\n",
    "\n",
    "    for j in range(len(output_)):\n",
    "        output_class.add_sentence(output_[j])\n",
    "        \n",
    "    return input_class,output_class,pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jZYOeQ74WEyk"
   },
   "outputs": [],
   "source": [
    "class language_loader(Dataset):\n",
    "    def __init__(self,pairs,input_lang,output_lang,device):\n",
    "        self.pairs = pairs\n",
    "        self.input_lang = input_lang\n",
    "        self.output_lang = output_lang\n",
    "        length_in = [len(pair[0]) for pair in self.pairs]\n",
    "        length_out = [len(pair[1]) for pair in self.pairs]\n",
    "\n",
    "        self.in_max = max(length_in)\n",
    "        self.out_max = max(length_out)\n",
    "        \n",
    "        self.device = device\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        pair = self.pairs[idx]\n",
    "        \n",
    "        input_s = pair[0].split(\" \")\n",
    "        input_s = input_s + ['EOS']\n",
    "        input_length = torch.LongTensor([len(input_s)])\n",
    "        \n",
    "        output_s = pair[1].split(\" \")\n",
    "        output_s = ['SOS'] + output_s + ['EOS']\n",
    "        output_length = torch.LongTensor([len(output_s)])\n",
    "        \n",
    "        src_pad_idx = self.input_lang.word2index['PAD']\n",
    "        trg_pad_idx = self.input_lang.word2index['PAD']\n",
    "        \n",
    "        input_tensor = torch.ones(self.in_max)*src_pad_idx\n",
    "        output_tensor = torch.ones(self.out_max)*trg_pad_idx\n",
    "        \n",
    "        for i in range(len(input_s)):\n",
    "            word = input_s[i]\n",
    "            input_tensor[i] = self.input_lang.word2index[word]\n",
    "        \n",
    "        for j in range(len(output_s)):\n",
    "            word = output_s[j]\n",
    "            output_tensor[j] = self.output_lang.word2index[word]\n",
    "\n",
    "        return input_tensor.long().to(self.device),output_tensor.long().to(self.device),input_length.to(self.device),output_length.to(self.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J8EZilOUPk3G"
   },
   "outputs": [],
   "source": [
    "in_,out_,p = read_Lang()\n",
    "l = language_loader(p,in_,out_,device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KSrmJCdTPoQa"
   },
   "source": [
    "# **Encoder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Td5TaUKdPnI2"
   },
   "outputs": [],
   "source": [
    "def clones(layer,N):\n",
    "    \"\"\"\n",
    "    Function that produces N identical layers and stores them in \n",
    "    a modulelist \n",
    "    \n",
    "    layer: The type of layer being passed in \n",
    "    N: the number of times that it is going to be repeated\n",
    "    \"\"\"\n",
    "    return nn.ModuleList([layer for _ in range(N)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "69AECs4GPvnR"
   },
   "source": [
    "**Attention Layer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oAY5CyVOPvAw"
   },
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self,num_head,embed,device,drop=0.1):\n",
    "        \"\"\"\n",
    "        num_head: number of heads of the attention layer\n",
    "        embed: the embedded dimension\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_head = num_head\n",
    "        self.embed = embed\n",
    "        # the dimension of each head\n",
    "        self.head_dim = int(self.embed / self.num_head)\n",
    "        \n",
    "        self.fc_q = nn.Linear(self.embed,self.embed)\n",
    "        self.fc_k = nn.Linear(self.embed,self.embed)\n",
    "        self.fc_v = nn.Linear(self.embed,self.embed)\n",
    "        \n",
    "        self.fc = nn.Linear(self.embed,self.embed)\n",
    "        self.device = device\n",
    "        self.dropout = nn.Dropout(drop)\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to(self.device)\n",
    "        \n",
    "    def forward(self,query,key,value,mask=None):\n",
    "        \"\"\"\n",
    "        query: of shape (batch_size,query_len,embed)\n",
    "        key: of shape (batch_size,key_len,embed)\n",
    "        value: of shape (batch_size,value_len,embed)\n",
    "        \"\"\"\n",
    "        src_len = query.shape[1]\n",
    "        batch_size = query.shape[0]\n",
    "\n",
    "        # Q is of shape (batch_size, query_len,embed)\n",
    "        # K is of shape (batch_size, key_len,embed)\n",
    "        # V is of shape (batch_size, value_len,embed)\n",
    "        Q = self.fc_q(query)\n",
    "        K = self.fc_k(key)\n",
    "        V = self.fc_v(value)\n",
    "        \n",
    "        \n",
    "        # Q will have shape (batch_size,num_heads, query_len,head_dim)\n",
    "        # K will have shape (batch_size,num_heads, key_len,head_dim)\n",
    "        # V will have shape (batch_size,num_heads, value_len,head_dim)\n",
    "        Q = Q.view(batch_size,-1,self.num_head,self.head_dim).permute(0, 2, 1, 3)\n",
    "        K = K.view(batch_size,-1,self.num_head,self.head_dim).permute(0, 2, 1, 3)\n",
    "        V = V.view(batch_size,-1,self.num_head,self.head_dim).permute(0, 2, 1, 3) \n",
    "        \n",
    "        #score has shape (batch_size, num_heads,query_len,key_len)\n",
    "        score = torch.matmul(Q,K.permute(0,1,3,2)) / self.scale\n",
    "        \n",
    "        \n",
    "        # check if masked is applied\n",
    "        if mask is not None:\n",
    "            score = score.masked_fill(mask == 0,value = torch.tensor(-1e10))\n",
    "        # attention is of shape (batch_size, num_heads,query_len,key_len)\n",
    "        attention = F.softmax(score,dim = -1)\n",
    "        \n",
    "        # key_len has to be equal to query_len\n",
    "        # output at each layer, x of shape (batch_size, num_heads,query_len,head_dim)\n",
    "        x = torch.matmul(self.dropout(attention),V)\n",
    "        \n",
    "        # x is of shape (batch_size,query_len,num_heads,head_dim)\n",
    "        x = x.permute(0,2,1,3)\n",
    "        \n",
    "        # x is of shape (batch_size,query_len,embed)\n",
    "        x = x.reshape(batch_size,src_len,self.embed)\n",
    "        \n",
    "        # x is of shape (batch_size,query_len,embed)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x,attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "A2W50gj_QnTJ",
    "outputId": "c33cf252-b6f4-41b4-a325-5b293fe68e1b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 2, 2, 5])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = Attention(2,20,'cpu')\n",
    "x = torch.randn(10,2,20)\n",
    "y = torch.randn(10,5,20)\n",
    "# should give the same shape as x\n",
    "p,q = a(x,y,y)\n",
    "q.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fFq4u52GQtvm"
   },
   "source": [
    "**Feed forward neural network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "il3bgCcMQs_9"
   },
   "outputs": [],
   "source": [
    "class ffnn(nn.Module):\n",
    "    def __init__(self,embed,hid_dim,device,drop=0.1):\n",
    "        super().__init__()\n",
    "        self.embed = embed\n",
    "        self.hid_dim = hid_dim\n",
    "        self.device = device\n",
    "        self.fc1 = nn.Linear(self.embed,self.hid_dim)\n",
    "        self.fc2 = nn.Linear(self.hid_dim,self.embed)\n",
    "        self.dropout = nn.Dropout(drop)\n",
    "    def forward(self,x):\n",
    "        \"\"\"\n",
    "        x : of shape (batch_size, src_len,embed)\n",
    "        \"\"\"\n",
    "        output1 = self.fc1(x)\n",
    "        output1 = output1.clamp(min=0)\n",
    "        output1 = self.dropout(output1)\n",
    "        \n",
    "        output2 = self.fc2(output1)\n",
    "        \n",
    "        return output2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "hNriAT79Q8mU",
    "outputId": "d2971af6-0453-4148-9629-b2b525c40a92"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 20])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fn = ffnn(20,100,'cpu')\n",
    "a = torch.randn(1,10,20)\n",
    "fn(a).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uhjC4bMPRAuj"
   },
   "source": [
    " **single encoder layer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WlxuJpVMRDXd"
   },
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self,num_head,embed,hid_dim,device,drop=0.1):\n",
    "        \"\"\"\n",
    "        num_head: number of heads in the multi-head self attention mechanism that we are using\n",
    "        head_dim: the dimension of each head\n",
    "        embed: the embedded size\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_head = num_head\n",
    "        self.embed = embed\n",
    "        self.hid_dim = hid_dim\n",
    "        self.drop = drop\n",
    "        self.device = device\n",
    "        # each layer has two sub-layers. The first is a multi-head \n",
    "        # self attention mechanism, the second is a simple, position-\n",
    "        # wise FC feed-forward network\n",
    "        \n",
    "        # attention network\n",
    "        self.attn = Attention(self.num_head,self.embed,self.device,drop=self.drop)\n",
    "        self.norm1 = nn.LayerNorm(self.embed)\n",
    "        \n",
    "        # feed forward neural network\n",
    "        self.ff = ffnn(self.embed,self.hid_dim,self.device,drop=self.drop)\n",
    "        self.norm2 = nn.LayerNorm(self.embed)\n",
    "        \n",
    "        self.dropout = nn.Dropout(drop)\n",
    "    def forward(self,x,mask=None):\n",
    "        # z1 of shape (batch_size,src_len,embed_dim)\n",
    "        z1,attention = self.attn(x,x,x,mask)\n",
    "\n",
    "        # sum and normalize, has shape (batch_size,src_len,embed_dim)\n",
    "        output1 = self.norm1(z1+self.dropout(x))\n",
    "\n",
    "        # feed into fully connected layer\n",
    "        output2 = self.ff(output1)\n",
    "        \n",
    "        # sum and normalize, has shape (batch_size,src_len,embed_dim)\n",
    "        output2 = self.norm2(output2+self.dropout(output1))\n",
    "        \n",
    "        return output2,attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "YKHQ72l3Rsmk",
    "outputId": "316c05ed-a115-46e8-97f6-c5a16dd2305c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 10, 20])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# input of shape num_samples,embed_dim\n",
    "NUM_HEAD = 2\n",
    "HID_DIM = 150\n",
    "EMBED = 20\n",
    "SRC_LEN = 10\n",
    "E = EncoderLayer(NUM_HEAD,EMBED,HID_DIM,'cpu',drop=0.2)\n",
    "_in = torch.randn(100,SRC_LEN,EMBED)\n",
    "E(_in)[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HTCy5y8_SFNq"
   },
   "source": [
    "# **Encoder compiled**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r7syJEi6SDny"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self,n_layers,input_size,num_head,embed,hid_dim,device,drop=0.1):\n",
    "        \"\"\"\n",
    "        n_layers: number of layers to repeat\n",
    "        input_size: the size of the dictionary being passed in\n",
    "        num_head: number of heads present in each multi-head self attention mechanism\n",
    "        head_dim: dimension of the each head \n",
    "        embed: embedded size of the dictionary\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.input_size = input_size\n",
    "        self.num_head = num_head\n",
    "        self.embed = embed\n",
    "        self.hid_dim = hid_dim\n",
    "        self.max_size = 200\n",
    "        self.device = device\n",
    "        self.drop = drop\n",
    "\n",
    "        # dictionary embedding and a positional embedding\n",
    "        self.embeddings = nn.Embedding(self.input_size,self.embed)\n",
    "        self.pos_embeddings = nn.Embedding(self.max_size,self.embed)\n",
    "        \n",
    "        self.layer = EncoderLayer(self.num_head,self.embed,self.hid_dim,self.device,self.drop)\n",
    "        self.layers = clones(self.layer,self.n_layers)\n",
    "        self.dropout = nn.Dropout(drop)\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([self.embed])).to(self.device)\n",
    "        \n",
    "    def forward(self,input_,mask=None):\n",
    "        \"\"\"\n",
    "        input_: of shape (batch_size,src_len) to indicate the position of words in the dictionary\n",
    "        \"\"\"\n",
    "        src_len = input_.shape[1]\n",
    "        batch_size = input_.shape[0]\n",
    "        \n",
    "        pos_idx = torch.arange(src_len).unsqueeze(0).repeat(batch_size,1).to(self.device)\n",
    "        pos_embed = self.pos_embeddings(pos_idx)\n",
    "\n",
    "        x = self.embeddings(input_)\n",
    "        x = self.dropout(x*self.scale+pos_embed)\n",
    "        \n",
    "        \n",
    "        for layer in self.layers:\n",
    "            x,attention = layer(x,mask)       \n",
    "        \n",
    "        return x,attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "8fJFqMqxSeoQ",
    "outputId": "139a0bd3-888e-4562-90c5-048c1456368e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 30])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# input of shape num_samples,embed_dim\n",
    "N = 6\n",
    "INPUT_SIZE = 200\n",
    "NUM_HEAD = 10\n",
    "HEAD_DIM = 30\n",
    "EMBED = 20\n",
    "\n",
    "x = torch.LongTensor([[0,2,3,4],[3,4,5,6]])\n",
    "E = Encoder(N,INPUT_SIZE,NUM_HEAD,HEAD_DIM,EMBED,'cpu')\n",
    "E(x)[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Rru5dyVoSuNt"
   },
   "source": [
    "# **Decoder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TlEn-hcdSv-r"
   },
   "outputs": [],
   "source": [
    "class Decoder_layer(nn.Module):\n",
    "    def __init__(self,num_head,embed,hid_dim,device,drop=0.1):\n",
    "        super().__init__()\n",
    "        self.num_head = num_head\n",
    "        self.embed = embed\n",
    "        self.hid_dim = hid_dim\n",
    "        self.drop = drop\n",
    "        self.device = device\n",
    "\n",
    "        self.attention1 = Attention(self.num_head,self.embed,device,drop=self.drop)\n",
    "        self.norm1 = nn.LayerNorm(self.embed)\n",
    "        \n",
    "        # This will also take into account the output from encoder\n",
    "        self.attention2 = Attention(self.num_head,self.embed,device,drop=self.drop)\n",
    "        self.norm2 = nn.LayerNorm(self.embed)\n",
    "        \n",
    "        self.ff = ffnn(self.embed,self.hid_dim,device,drop=self.drop)\n",
    "        self.norm3 = nn.LayerNorm(self.embed)\n",
    "        self.dropout = nn.Dropout(drop)\n",
    "        \n",
    "    def forward(self,trg,src,trg_mask=None,src_mask=None):\n",
    "        \"\"\"\n",
    "        trg: of shape (batch_size,trg_len,embed)\n",
    "        encoder_input: of shape (batch_size,src_len,embed) => Q,V\n",
    "        \"\"\"\n",
    "        # First pass through the self attention layer of decoder\n",
    "        # selfattn is of shape (batch_size,trg_len,embed)\n",
    "        output1,_ = self.attention1(trg,trg,trg,trg_mask)\n",
    "        \n",
    "        # first layer normalization\n",
    "        output1 = self.norm1(trg+self.dropout(output1))\n",
    "        \n",
    "        # encoder_decoder attention layer\n",
    "        # attention is of shape (batch_size,num_head,trg_len,src_Len)\n",
    "        output2,attention = self.attention2(output1,src,src,src_mask)\n",
    "        \n",
    "        # second layer normalization\n",
    "        output2 = self.norm2(output1 + self.dropout(output2))\n",
    "        \n",
    "        # feed forward neural network\n",
    "        ff = self.ff(output2)\n",
    "        \n",
    "        output3 = self.norm3(output2+ self.dropout(ff))\n",
    "        \n",
    "        return output3,attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "erf29_28S45q",
    "outputId": "15a2eda2-8d7c-4ef1-8f70-ea61de26a746"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([15, 2, 1, 5])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_HEAD = 2\n",
    "SRC_LEN = 5\n",
    "TRG_LEN = 1\n",
    "HID_DIM = 120\n",
    "EMBED = 50\n",
    "ENCODER_INPUT = torch.randn(15,SRC_LEN,EMBED)\n",
    "x = torch.randn(15,TRG_LEN,EMBED)\n",
    "d = Decoder_layer(NUM_HEAD,EMBED,HID_DIM,'cpu',drop=0.5)\n",
    "d(x,ENCODER_INPUT)[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bpRhjxLgTGsi"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self,n_layers,output_size,num_head,embed,hid_dim,device,drop=0.1):\n",
    "        super().__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.num_head = num_head\n",
    "        self.embed = embed\n",
    "        self.hid_dim = hid_dim\n",
    "        self.output_size = output_size\n",
    "        self.device = device\n",
    "        self.drop = drop\n",
    "\n",
    "        layer = Decoder_layer(self.num_head,self.embed,self.hid_dim,device,drop=self.drop)\n",
    "        self.layers = clones(layer,self.n_layers)\n",
    "        self.max_size = 200\n",
    "        \n",
    "        self.embedding = nn.Embedding(self.output_size,self.embed)\n",
    "        self.pos_embedding = nn.Embedding(self.max_size,self.embed)\n",
    "        \n",
    "        self.fc_out = nn.Linear(embed,output_size)\n",
    "        self.dropout = nn.Dropout(drop)\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([self.embed])).to(self.device)\n",
    "        \n",
    "    def forward(self,input_,encoder_input,trg_mask=None,src_mask=None):\n",
    "        \"\"\"\n",
    "        input_: (batch_size,trg_len)\n",
    "        encoder_input: (batch_size,trg_len,embed)\n",
    "        \"\"\"\n",
    "        batch_size = input_.shape[0]\n",
    "        trg_len = input_.shape[1]\n",
    "\n",
    "        x = self.embedding(input_)\n",
    "        \n",
    "        pos_idx = torch.arange(trg_len).unsqueeze(0).repeat(batch_size,1).to(self.device)\n",
    "        pos_embedding = self.pos_embedding(pos_idx)\n",
    "\n",
    "        x = self.dropout(x*self.scale + pos_embedding)\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            x,attention = layer(x,encoder_input,trg_mask,src_mask)\n",
    "            \n",
    "        # x is of shape (1,trg_len,output)\n",
    "        x = self.fc_out(x)\n",
    "        return x,attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "GPNnT_XoTyjH",
    "outputId": "15a203f6-6e53-47a1-a52b-6f43f1915dd5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 100])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_HEAD = 2\n",
    "SRC_LEN = 5\n",
    "TRG_LEN = 2\n",
    "HID_DIM = 120\n",
    "EMBED = 50\n",
    "OUTPUT_SIZE = 100\n",
    "N_LAYER = 5\n",
    "ENCODER_INPUT = torch.randn(2,SRC_LEN,EMBED)\n",
    "x = torch.LongTensor([[1,2],[2,3]])\n",
    "de = Decoder(N_LAYER,OUTPUT_SIZE,NUM_HEAD,EMBED,HID_DIM,'cpu',drop=0.3)\n",
    "de(x,ENCODER_INPUT)[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vt1b8TBVUrTm"
   },
   "source": [
    "# **Full Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7lNI4azwUqEZ"
   },
   "outputs": [],
   "source": [
    "class self_attn(nn.Module):\n",
    "    def __init__(self,input_lang,output_lang,n_layers,num_head,embed,hid_dim,device,drop=0.1):\n",
    "        super().__init__()\n",
    "        self.input_lang = input_lang\n",
    "        self.output_lang = output_lang\n",
    "        \n",
    "        self.num_head = num_head\n",
    "        self.embed = embed\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.input_size = self.input_lang.n_words\n",
    "        self.output_size = self.output_lang.n_words\n",
    "        self.drop = drop\n",
    "        self.device = device\n",
    "\n",
    "        self.SRC_PAD_IDX = self.input_lang.word2index['PAD']\n",
    "        self.TRG_PAD_IDX = self.output_lang.word2index['PAD']\n",
    "\n",
    "        self.encoder = Encoder(self.n_layers,self.input_size,self.num_head,self.embed,self.hid_dim,self.device,drop=self.drop)\n",
    "        self.decoder = Decoder(self.n_layers,self.output_size,self.num_head,self.embed,self.hid_dim,self.device,drop=self.drop)\n",
    "\n",
    "        self.loss = nn.CrossEntropyLoss(ignore_index=self.TRG_PAD_IDX)\n",
    "\n",
    "    def make_src_mask(self,input_t):\n",
    "        # input_t = [batch_size, src_len]\n",
    "        src_mask = (input_t != self.SRC_PAD_IDX).unsqueeze(1).unsqueeze(2)\n",
    "        # src_mask = [batch_size,1,1,src_len]\n",
    "\n",
    "        return src_mask\n",
    "\n",
    "    def make_trg_mask(self,output_t):\n",
    "         # output_t = [batch_size,trg len]\n",
    "        trg_mask = (output_t !=  self.TRG_PAD_IDX).unsqueeze(1).unsqueeze(2)\n",
    "        # trg_mask = [batch_size , 1, 1,trg len]\n",
    "\n",
    "        trg_len = output_t.shape[1]\n",
    "\n",
    "        trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), device = self.device)).bool()\n",
    "        \n",
    "        trg_mask = trg_mask & trg_sub_mask\n",
    "        return trg_mask\n",
    "    \n",
    "    def forward(self,data):\n",
    "        # input_t = [batch_size, in_max_len]\n",
    "        # output_t = [batch_size, out_max_len]\n",
    "        input_t,output_t,input_len,output_len = data\n",
    "        \n",
    "        largest_in_len = torch.max(input_len)\n",
    "        largest_out_len = torch.max(output_len)\n",
    "\n",
    "        input_t = input_t[:,:largest_in_len]\n",
    "        output_t = output_t[:,:largest_out_len]\n",
    "        \n",
    "        src_mask = self.make_src_mask(input_t)\n",
    "        trg_mask = self.make_trg_mask(output_t[:,:-1])\n",
    "\n",
    "        # src_output of shape (batch_size, src_len,embed)\n",
    "        src_output,enc_attn = self.encoder(input_t,src_mask)\n",
    "        \n",
    "        # trg_output of shape (batch_size, trg_len,output_size)\n",
    "        trg_output,dec_attn = self.decoder(output_t[:,:-1],src_output,trg_mask,src_mask)\n",
    "        \n",
    "        trg_output = trg_output.contiguous().view(-1,self.output_size)\n",
    "        output_t = output_t[:,1:].contiguous().view(-1)\n",
    "\n",
    "        loss = self.loss(trg_output,output_t)\n",
    "        return loss,(enc_attn,dec_attn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MAJotjjkbDGZ"
   },
   "outputs": [],
   "source": [
    "INPUT_LANG,OUTPUT_LANG, pairs= read_Lang()\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits=10,shuffle=True)\n",
    "pairs = np.array(pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gTdQ56cBeIDB"
   },
   "source": [
    "# **Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GlDd3Z02bLa7"
   },
   "outputs": [],
   "source": [
    "def train(model,trainloader,optimizer):\n",
    "    clip = 1\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    model.train()\n",
    "    for data in trainloader:\n",
    "        # zero out gradient descent\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass\n",
    "\n",
    "        # trg_output of shape (batch_size, trg_len,output_size)   \n",
    "        loss,_ = model(data)            \n",
    "\n",
    "        # call backward on loss\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(),clip)\n",
    "                \n",
    "        # perform gradient descent\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    avg_loss = epoch_loss/len(trainloader)\n",
    "    \n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HVwzxTiFeKoq"
   },
   "source": [
    "# **Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-JwkFwJoeMOu"
   },
   "outputs": [],
   "source": [
    "def evaluation(model,testloader):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        running_loss = 0\n",
    "        for data in testloader:\n",
    "            loss,_ = model(data)\n",
    "\n",
    "            running_loss += loss.item()\n",
    "                \n",
    "            \n",
    "        return running_loss/len(testloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ql8Px6XHmLjN"
   },
   "source": [
    "# **Translation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gpWk8r7DmOCM"
   },
   "outputs": [],
   "source": [
    "def translate(model,input_,device,maxIter=50):\n",
    "    input_ = input_.lower()\n",
    "    input_ = re.sub(r\"([.!?])\",r\" \\1\",input_)\n",
    "    input_ =  re.sub(r\"[^a-zA-Z.!?]+\", r\" \",input_)\n",
    "    \n",
    "    input_s = input_.split(\" \")\n",
    "    input_s = input_s + ['EOS']    \n",
    "    input_t = torch.zeros(1,len(input_s)).to(device)\n",
    "    for i in range(len(input_s)):\n",
    "        idx_i = model.input_lang.word2index[input_s[i]]\n",
    "        input_t[:,i] = idx_i\n",
    "    input_t = input_t.long()\n",
    "\n",
    "    src_mask = model.make_src_mask(input_t)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        src_outputs,src_attention = model.encoder(input_t,src_mask)    \n",
    "        \n",
    "        # the first word passed into the decoder network is the SOS\n",
    "        decoder_input = model.output_lang.word2index['SOS']\n",
    "        decoder_input = [decoder_input]\n",
    "            \n",
    "        iter_ = 0\n",
    "        attention = []\n",
    "        word = []\n",
    "        \n",
    "        for i in range(maxIter):\n",
    "            trg_tensor = torch.LongTensor(decoder_input).unsqueeze(0).to(device)\n",
    "            \n",
    "            trg_mask = model.make_trg_mask(trg_tensor)\n",
    "\n",
    "            # trg_output = [batch_size, trg_len, output_size]\n",
    "            trg_output,trg_att = model.decoder(trg_tensor,src_outputs,trg_mask,src_mask)\n",
    "            \n",
    "            attention.append(trg_att.flatten())\n",
    "                \n",
    "            # find the top scoring candidate\n",
    "            top1 = torch.argmax(trg_output[0],dim=1)\n",
    "            \n",
    "            # find the current token corresponding to the top scoring candiate\n",
    "            curr_token = model.output_lang.index2word[top1[-1].item()]\n",
    "\n",
    "            \n",
    "            if curr_token == 'EOS':\n",
    "                break\n",
    "            word.append(curr_token)\n",
    "\n",
    "            # update decoder output to the top candidate\n",
    "            decoder_input.append(top1[-1].item())\n",
    "            \n",
    "            iter_ +=1\n",
    "        if model.output_lang.name == 'eng':\n",
    "            word = \" \".join(word)\n",
    "        else:\n",
    "            word = \"\".join(word)\n",
    "    return word,attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WjLnMVc3eYMt"
   },
   "outputs": [],
   "source": [
    "HIDDEN_SIZE = 512\n",
    "EMBEDDED_SIZE = 256\n",
    "N_LAYERS = 3\n",
    "NUM_HEAD = 8\n",
    "DROP_P = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Mm4NbYepfSUB",
    "outputId": "3619795e-6ec6-4e0d-cfbc-570760992fc1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At validation 1, iteration 1, the train loss is 4.496 and test loss is 3.647,time it takes is 11.741\n",
      "At validation 1, iteration 2, the train loss is 3.420 and test loss is 3.130,time it takes is 11.743\n",
      "At validation 1, iteration 3, the train loss is 2.963 and test loss is 2.874,time it takes is 11.738\n",
      "At validation 1, iteration 4, the train loss is 2.654 and test loss is 2.694,time it takes is 11.610\n",
      "At validation 1, iteration 5, the train loss is 2.426 and test loss is 2.571,time it takes is 11.662\n",
      "At validation 1, iteration 6, the train loss is 2.240 and test loss is 2.521,time it takes is 11.612\n",
      "At validation 1, iteration 7, the train loss is 2.092 and test loss is 2.467,time it takes is 11.680\n",
      "At validation 1, iteration 8, the train loss is 1.962 and test loss is 2.412,time it takes is 11.686\n",
      "At validation 1, iteration 9, the train loss is 1.857 and test loss is 2.374,time it takes is 11.701\n",
      "At validation 1, iteration 10, the train loss is 1.775 and test loss is 2.353,time it takes is 11.854\n",
      "At validation 1, iteration 11, the train loss is 1.686 and test loss is 2.333,time it takes is 11.628\n",
      "At validation 1, iteration 12, the train loss is 1.619 and test loss is 2.337,time it takes is 11.655\n",
      "At validation 1, iteration 13, the train loss is 1.555 and test loss is 2.334,time it takes is 11.493\n",
      "At validation 1, iteration 14, the train loss is 1.499 and test loss is 2.321,time it takes is 11.528\n",
      "At validation 1, iteration 15, the train loss is 1.450 and test loss is 2.322,time it takes is 11.420\n",
      "At validation 1, iteration 16, the train loss is 1.411 and test loss is 2.336,time it takes is 11.601\n",
      "At validation 1, iteration 17, the train loss is 1.368 and test loss is 2.338,time it takes is 11.608\n",
      "At validation 1, iteration 18, the train loss is 1.326 and test loss is 2.342,time it takes is 11.423\n",
      "At validation 1, iteration 19, the train loss is 1.292 and test loss is 2.362,time it takes is 11.652\n",
      "At validation 1, iteration 20, the train loss is 1.263 and test loss is 2.371,time it takes is 11.482\n",
      "At validation 2, iteration 1, the train loss is 4.469 and test loss is 3.624,time it takes is 11.585\n",
      "At validation 2, iteration 2, the train loss is 3.406 and test loss is 3.155,time it takes is 11.660\n",
      "At validation 2, iteration 3, the train loss is 2.955 and test loss is 2.918,time it takes is 11.575\n",
      "At validation 2, iteration 4, the train loss is 2.653 and test loss is 2.713,time it takes is 11.905\n",
      "At validation 2, iteration 5, the train loss is 2.426 and test loss is 2.606,time it takes is 11.927\n",
      "At validation 2, iteration 6, the train loss is 2.235 and test loss is 2.535,time it takes is 11.863\n",
      "At validation 2, iteration 7, the train loss is 2.090 and test loss is 2.451,time it takes is 11.933\n",
      "At validation 2, iteration 8, the train loss is 1.964 and test loss is 2.407,time it takes is 12.051\n",
      "At validation 2, iteration 9, the train loss is 1.857 and test loss is 2.369,time it takes is 11.743\n",
      "At validation 2, iteration 10, the train loss is 1.774 and test loss is 2.356,time it takes is 11.752\n",
      "At validation 2, iteration 11, the train loss is 1.693 and test loss is 2.334,time it takes is 11.683\n",
      "At validation 2, iteration 12, the train loss is 1.623 and test loss is 2.318,time it takes is 11.798\n",
      "At validation 2, iteration 13, the train loss is 1.558 and test loss is 2.328,time it takes is 11.663\n",
      "At validation 2, iteration 14, the train loss is 1.504 and test loss is 2.318,time it takes is 11.626\n",
      "At validation 2, iteration 15, the train loss is 1.451 and test loss is 2.323,time it takes is 11.689\n",
      "At validation 2, iteration 16, the train loss is 1.406 and test loss is 2.329,time it takes is 11.706\n",
      "At validation 2, iteration 17, the train loss is 1.373 and test loss is 2.332,time it takes is 11.745\n",
      "At validation 2, iteration 18, the train loss is 1.324 and test loss is 2.327,time it takes is 11.614\n",
      "At validation 2, iteration 19, the train loss is 1.299 and test loss is 2.354,time it takes is 11.674\n",
      "At validation 2, iteration 20, the train loss is 1.263 and test loss is 2.337,time it takes is 11.632\n",
      "At validation 3, iteration 1, the train loss is 4.477 and test loss is 3.626,time it takes is 11.686\n",
      "At validation 3, iteration 2, the train loss is 3.408 and test loss is 3.147,time it takes is 11.670\n",
      "At validation 3, iteration 3, the train loss is 2.957 and test loss is 2.861,time it takes is 11.685\n",
      "At validation 3, iteration 4, the train loss is 2.655 and test loss is 2.702,time it takes is 11.642\n",
      "At validation 3, iteration 5, the train loss is 2.423 and test loss is 2.581,time it takes is 11.752\n",
      "At validation 3, iteration 6, the train loss is 2.243 and test loss is 2.518,time it takes is 11.656\n",
      "At validation 3, iteration 7, the train loss is 2.088 and test loss is 2.424,time it takes is 11.639\n",
      "At validation 3, iteration 8, the train loss is 1.958 and test loss is 2.398,time it takes is 11.805\n",
      "At validation 3, iteration 9, the train loss is 1.858 and test loss is 2.361,time it takes is 11.650\n",
      "At validation 3, iteration 10, the train loss is 1.767 and test loss is 2.352,time it takes is 11.743\n",
      "At validation 3, iteration 11, the train loss is 1.690 and test loss is 2.314,time it takes is 11.791\n",
      "At validation 3, iteration 12, the train loss is 1.619 and test loss is 2.308,time it takes is 11.681\n",
      "At validation 3, iteration 13, the train loss is 1.552 and test loss is 2.313,time it takes is 11.716\n",
      "At validation 3, iteration 14, the train loss is 1.503 and test loss is 2.317,time it takes is 11.847\n",
      "At validation 3, iteration 15, the train loss is 1.449 and test loss is 2.307,time it takes is 11.724\n",
      "At validation 3, iteration 16, the train loss is 1.407 and test loss is 2.312,time it takes is 11.713\n",
      "At validation 3, iteration 17, the train loss is 1.371 and test loss is 2.303,time it takes is 11.620\n",
      "At validation 3, iteration 18, the train loss is 1.325 and test loss is 2.324,time it takes is 11.660\n",
      "At validation 3, iteration 19, the train loss is 1.281 and test loss is 2.328,time it takes is 11.690\n",
      "At validation 3, iteration 20, the train loss is 1.255 and test loss is 2.343,time it takes is 11.589\n",
      "At validation 4, iteration 1, the train loss is 4.484 and test loss is 3.605,time it takes is 11.423\n",
      "At validation 4, iteration 2, the train loss is 3.407 and test loss is 3.138,time it takes is 11.471\n",
      "At validation 4, iteration 3, the train loss is 2.959 and test loss is 2.857,time it takes is 11.503\n",
      "At validation 4, iteration 4, the train loss is 2.647 and test loss is 2.701,time it takes is 11.549\n",
      "At validation 4, iteration 5, the train loss is 2.423 and test loss is 2.588,time it takes is 11.676\n",
      "At validation 4, iteration 6, the train loss is 2.238 and test loss is 2.491,time it takes is 11.479\n",
      "At validation 4, iteration 7, the train loss is 2.088 and test loss is 2.434,time it takes is 11.490\n",
      "At validation 4, iteration 8, the train loss is 1.964 and test loss is 2.399,time it takes is 11.530\n",
      "At validation 4, iteration 9, the train loss is 1.856 and test loss is 2.367,time it takes is 11.453\n",
      "At validation 4, iteration 10, the train loss is 1.764 and test loss is 2.347,time it takes is 11.459\n",
      "At validation 4, iteration 11, the train loss is 1.679 and test loss is 2.336,time it takes is 11.583\n",
      "At validation 4, iteration 12, the train loss is 1.608 and test loss is 2.333,time it takes is 11.549\n",
      "At validation 4, iteration 13, the train loss is 1.553 and test loss is 2.329,time it takes is 11.516\n",
      "At validation 4, iteration 14, the train loss is 1.491 and test loss is 2.327,time it takes is 11.392\n",
      "At validation 4, iteration 15, the train loss is 1.441 and test loss is 2.316,time it takes is 11.424\n",
      "At validation 4, iteration 16, the train loss is 1.393 and test loss is 2.332,time it takes is 11.519\n",
      "At validation 4, iteration 17, the train loss is 1.353 and test loss is 2.340,time it takes is 11.469\n",
      "At validation 4, iteration 18, the train loss is 1.324 and test loss is 2.342,time it takes is 11.421\n",
      "At validation 4, iteration 19, the train loss is 1.293 and test loss is 2.348,time it takes is 11.572\n",
      "At validation 4, iteration 20, the train loss is 1.266 and test loss is 2.358,time it takes is 11.537\n",
      "At validation 5, iteration 1, the train loss is 4.464 and test loss is 3.617,time it takes is 11.853\n",
      "At validation 5, iteration 2, the train loss is 3.402 and test loss is 3.141,time it takes is 11.904\n",
      "At validation 5, iteration 3, the train loss is 2.951 and test loss is 2.890,time it takes is 11.849\n",
      "At validation 5, iteration 4, the train loss is 2.648 and test loss is 2.712,time it takes is 11.876\n",
      "At validation 5, iteration 5, the train loss is 2.423 and test loss is 2.616,time it takes is 11.893\n",
      "At validation 5, iteration 6, the train loss is 2.240 and test loss is 2.526,time it takes is 11.967\n",
      "At validation 5, iteration 7, the train loss is 2.093 and test loss is 2.469,time it takes is 11.968\n",
      "At validation 5, iteration 8, the train loss is 1.966 and test loss is 2.432,time it takes is 11.944\n",
      "At validation 5, iteration 9, the train loss is 1.863 and test loss is 2.395,time it takes is 11.923\n",
      "At validation 5, iteration 10, the train loss is 1.770 and test loss is 2.396,time it takes is 11.904\n",
      "At validation 5, iteration 11, the train loss is 1.694 and test loss is 2.372,time it takes is 11.982\n",
      "At validation 5, iteration 12, the train loss is 1.626 and test loss is 2.361,time it takes is 12.082\n",
      "At validation 5, iteration 13, the train loss is 1.562 and test loss is 2.363,time it takes is 11.898\n",
      "At validation 5, iteration 14, the train loss is 1.503 and test loss is 2.356,time it takes is 11.908\n",
      "At validation 5, iteration 15, the train loss is 1.451 and test loss is 2.355,time it takes is 11.999\n",
      "At validation 5, iteration 16, the train loss is 1.416 and test loss is 2.356,time it takes is 11.892\n",
      "At validation 5, iteration 17, the train loss is 1.367 and test loss is 2.373,time it takes is 12.007\n",
      "At validation 5, iteration 18, the train loss is 1.327 and test loss is 2.365,time it takes is 11.971\n",
      "At validation 5, iteration 19, the train loss is 1.291 and test loss is 2.375,time it takes is 12.130\n",
      "At validation 5, iteration 20, the train loss is 1.263 and test loss is 2.397,time it takes is 11.914\n",
      "At validation 6, iteration 1, the train loss is 4.482 and test loss is 3.594,time it takes is 11.866\n",
      "At validation 6, iteration 2, the train loss is 3.427 and test loss is 3.159,time it takes is 11.886\n",
      "At validation 6, iteration 3, the train loss is 2.976 and test loss is 2.829,time it takes is 11.879\n",
      "At validation 6, iteration 4, the train loss is 2.672 and test loss is 2.659,time it takes is 11.839\n",
      "At validation 6, iteration 5, the train loss is 2.445 and test loss is 2.556,time it takes is 11.971\n",
      "At validation 6, iteration 6, the train loss is 2.259 and test loss is 2.451,time it takes is 11.856\n",
      "At validation 6, iteration 7, the train loss is 2.108 and test loss is 2.406,time it takes is 11.848\n",
      "At validation 6, iteration 8, the train loss is 1.983 and test loss is 2.377,time it takes is 11.838\n",
      "At validation 6, iteration 9, the train loss is 1.876 and test loss is 2.339,time it takes is 11.782\n",
      "At validation 6, iteration 10, the train loss is 1.775 and test loss is 2.305,time it takes is 11.689\n",
      "At validation 6, iteration 11, the train loss is 1.697 and test loss is 2.293,time it takes is 11.786\n",
      "At validation 6, iteration 12, the train loss is 1.628 and test loss is 2.281,time it takes is 11.620\n",
      "At validation 6, iteration 13, the train loss is 1.557 and test loss is 2.279,time it takes is 11.707\n",
      "At validation 6, iteration 14, the train loss is 1.511 and test loss is 2.260,time it takes is 11.622\n",
      "At validation 6, iteration 15, the train loss is 1.458 and test loss is 2.286,time it takes is 11.518\n",
      "At validation 6, iteration 16, the train loss is 1.419 and test loss is 2.262,time it takes is 11.522\n",
      "At validation 6, iteration 17, the train loss is 1.366 and test loss is 2.272,time it takes is 11.491\n",
      "At validation 6, iteration 18, the train loss is 1.329 and test loss is 2.285,time it takes is 11.486\n",
      "At validation 6, iteration 19, the train loss is 1.303 and test loss is 2.281,time it takes is 11.524\n",
      "At validation 6, iteration 20, the train loss is 1.274 and test loss is 2.274,time it takes is 11.591\n",
      "At validation 7, iteration 1, the train loss is 4.452 and test loss is 3.656,time it takes is 11.492\n",
      "At validation 7, iteration 2, the train loss is 3.408 and test loss is 3.188,time it takes is 11.429\n",
      "At validation 7, iteration 3, the train loss is 2.966 and test loss is 2.931,time it takes is 11.529\n",
      "At validation 7, iteration 4, the train loss is 2.659 and test loss is 2.748,time it takes is 11.444\n",
      "At validation 7, iteration 5, the train loss is 2.425 and test loss is 2.638,time it takes is 11.385\n",
      "At validation 7, iteration 6, the train loss is 2.246 and test loss is 2.556,time it takes is 11.364\n",
      "At validation 7, iteration 7, the train loss is 2.097 and test loss is 2.498,time it takes is 11.397\n",
      "At validation 7, iteration 8, the train loss is 1.967 and test loss is 2.461,time it takes is 11.342\n",
      "At validation 7, iteration 9, the train loss is 1.862 and test loss is 2.414,time it takes is 11.328\n",
      "At validation 7, iteration 10, the train loss is 1.772 and test loss is 2.412,time it takes is 11.299\n",
      "At validation 7, iteration 11, the train loss is 1.692 and test loss is 2.402,time it takes is 11.297\n",
      "At validation 7, iteration 12, the train loss is 1.617 and test loss is 2.381,time it takes is 11.377\n",
      "At validation 7, iteration 13, the train loss is 1.550 and test loss is 2.387,time it takes is 11.294\n",
      "At validation 7, iteration 14, the train loss is 1.501 and test loss is 2.388,time it takes is 11.415\n",
      "At validation 7, iteration 15, the train loss is 1.448 and test loss is 2.382,time it takes is 11.474\n",
      "At validation 7, iteration 16, the train loss is 1.398 and test loss is 2.393,time it takes is 11.263\n",
      "At validation 7, iteration 17, the train loss is 1.365 and test loss is 2.395,time it takes is 11.181\n",
      "At validation 7, iteration 18, the train loss is 1.330 and test loss is 2.403,time it takes is 11.341\n",
      "At validation 7, iteration 19, the train loss is 1.288 and test loss is 2.392,time it takes is 11.186\n",
      "At validation 7, iteration 20, the train loss is 1.262 and test loss is 2.409,time it takes is 11.245\n",
      "At validation 8, iteration 1, the train loss is 4.468 and test loss is 3.621,time it takes is 11.354\n",
      "At validation 8, iteration 2, the train loss is 3.405 and test loss is 3.149,time it takes is 11.202\n",
      "At validation 8, iteration 3, the train loss is 2.955 and test loss is 2.868,time it takes is 11.183\n",
      "At validation 8, iteration 4, the train loss is 2.651 and test loss is 2.694,time it takes is 11.208\n",
      "At validation 8, iteration 5, the train loss is 2.422 and test loss is 2.593,time it takes is 11.166\n",
      "At validation 8, iteration 6, the train loss is 2.239 and test loss is 2.516,time it takes is 11.304\n",
      "At validation 8, iteration 7, the train loss is 2.087 and test loss is 2.447,time it takes is 11.163\n",
      "At validation 8, iteration 8, the train loss is 1.965 and test loss is 2.421,time it takes is 11.164\n",
      "At validation 8, iteration 9, the train loss is 1.849 and test loss is 2.385,time it takes is 11.166\n",
      "At validation 8, iteration 10, the train loss is 1.767 and test loss is 2.355,time it takes is 11.199\n",
      "At validation 8, iteration 11, the train loss is 1.688 and test loss is 2.347,time it takes is 11.181\n",
      "At validation 8, iteration 12, the train loss is 1.613 and test loss is 2.328,time it takes is 11.124\n",
      "At validation 8, iteration 13, the train loss is 1.551 and test loss is 2.331,time it takes is 11.211\n",
      "At validation 8, iteration 14, the train loss is 1.495 and test loss is 2.330,time it takes is 11.139\n",
      "At validation 8, iteration 15, the train loss is 1.448 and test loss is 2.323,time it takes is 11.186\n",
      "At validation 8, iteration 16, the train loss is 1.406 and test loss is 2.327,time it takes is 11.259\n",
      "At validation 8, iteration 17, the train loss is 1.355 and test loss is 2.346,time it takes is 11.132\n",
      "At validation 8, iteration 18, the train loss is 1.325 and test loss is 2.342,time it takes is 11.160\n",
      "At validation 8, iteration 19, the train loss is 1.293 and test loss is 2.334,time it takes is 11.162\n",
      "At validation 8, iteration 20, the train loss is 1.258 and test loss is 2.348,time it takes is 11.146\n",
      "At validation 9, iteration 1, the train loss is 4.473 and test loss is 3.644,time it takes is 11.128\n",
      "At validation 9, iteration 2, the train loss is 3.406 and test loss is 3.165,time it takes is 11.407\n",
      "At validation 9, iteration 3, the train loss is 2.955 and test loss is 2.875,time it takes is 11.151\n",
      "At validation 9, iteration 4, the train loss is 2.654 and test loss is 2.719,time it takes is 11.055\n",
      "At validation 9, iteration 5, the train loss is 2.429 and test loss is 2.588,time it takes is 11.087\n",
      "At validation 9, iteration 6, the train loss is 2.244 and test loss is 2.503,time it takes is 11.134\n",
      "At validation 9, iteration 7, the train loss is 2.095 and test loss is 2.445,time it takes is 11.096\n",
      "At validation 9, iteration 8, the train loss is 1.962 and test loss is 2.416,time it takes is 11.106\n",
      "At validation 9, iteration 9, the train loss is 1.865 and test loss is 2.384,time it takes is 11.144\n",
      "At validation 9, iteration 10, the train loss is 1.772 and test loss is 2.370,time it takes is 11.159\n",
      "At validation 9, iteration 11, the train loss is 1.688 and test loss is 2.344,time it takes is 11.075\n",
      "At validation 9, iteration 12, the train loss is 1.619 and test loss is 2.340,time it takes is 11.073\n",
      "At validation 9, iteration 13, the train loss is 1.558 and test loss is 2.339,time it takes is 11.101\n",
      "At validation 9, iteration 14, the train loss is 1.497 and test loss is 2.323,time it takes is 11.121\n",
      "At validation 9, iteration 15, the train loss is 1.447 and test loss is 2.332,time it takes is 11.136\n",
      "At validation 9, iteration 16, the train loss is 1.403 and test loss is 2.344,time it takes is 11.104\n",
      "At validation 9, iteration 17, the train loss is 1.364 and test loss is 2.343,time it takes is 11.174\n",
      "At validation 9, iteration 18, the train loss is 1.325 and test loss is 2.341,time it takes is 11.219\n",
      "At validation 9, iteration 19, the train loss is 1.288 and test loss is 2.348,time it takes is 11.060\n",
      "At validation 9, iteration 20, the train loss is 1.263 and test loss is 2.354,time it takes is 11.051\n",
      "At validation 10, iteration 1, the train loss is 4.474 and test loss is 3.617,time it takes is 11.063\n",
      "At validation 10, iteration 2, the train loss is 3.416 and test loss is 3.156,time it takes is 11.110\n",
      "At validation 10, iteration 3, the train loss is 2.965 and test loss is 2.870,time it takes is 11.185\n",
      "At validation 10, iteration 4, the train loss is 2.662 and test loss is 2.703,time it takes is 11.142\n",
      "At validation 10, iteration 5, the train loss is 2.432 and test loss is 2.576,time it takes is 11.130\n",
      "At validation 10, iteration 6, the train loss is 2.246 and test loss is 2.483,time it takes is 11.089\n",
      "At validation 10, iteration 7, the train loss is 2.096 and test loss is 2.426,time it takes is 11.158\n",
      "At validation 10, iteration 8, the train loss is 1.974 and test loss is 2.421,time it takes is 11.228\n",
      "At validation 10, iteration 9, the train loss is 1.867 and test loss is 2.380,time it takes is 11.194\n",
      "At validation 10, iteration 10, the train loss is 1.766 and test loss is 2.360,time it takes is 11.272\n",
      "At validation 10, iteration 11, the train loss is 1.687 and test loss is 2.340,time it takes is 11.206\n",
      "At validation 10, iteration 12, the train loss is 1.622 and test loss is 2.329,time it takes is 11.178\n",
      "At validation 10, iteration 13, the train loss is 1.558 and test loss is 2.342,time it takes is 11.333\n",
      "At validation 10, iteration 14, the train loss is 1.495 and test loss is 2.324,time it takes is 11.475\n",
      "At validation 10, iteration 15, the train loss is 1.452 and test loss is 2.324,time it takes is 11.376\n",
      "At validation 10, iteration 16, the train loss is 1.398 and test loss is 2.322,time it takes is 11.458\n",
      "At validation 10, iteration 17, the train loss is 1.363 and test loss is 2.332,time it takes is 11.529\n",
      "At validation 10, iteration 18, the train loss is 1.322 and test loss is 2.326,time it takes is 11.466\n",
      "At validation 10, iteration 19, the train loss is 1.286 and test loss is 2.343,time it takes is 11.208\n",
      "At validation 10, iteration 20, the train loss is 1.254 and test loss is 2.336,time it takes is 11.218\n"
     ]
    }
   ],
   "source": [
    "EPOCH = 20\n",
    "best_val_loss = float('inf')\n",
    "num_val = 1\n",
    "\n",
    "for train_idx, test_idx in kf.split(pairs):\n",
    "  train_pairs,test_pairs = pairs[train_idx],pairs[test_idx]\n",
    "  trainloader = language_loader(train_pairs,INPUT_LANG,OUTPUT_LANG,device)\n",
    "  trainloader = DataLoader(trainloader,shuffle=True,batch_size=128)\n",
    "\n",
    "  testloader = language_loader(test_pairs,INPUT_LANG,OUTPUT_LANG,device)\n",
    "  testloader = DataLoader(testloader,batch_size=128)\n",
    "  \n",
    "\n",
    "  train_loss_vec  = []\n",
    "  test_loss_vec = []\n",
    "\n",
    "  model = self_attn(INPUT_LANG,OUTPUT_LANG,N_LAYERS,NUM_HEAD,EMBEDDED_SIZE,HIDDEN_SIZE,device,DROP_P)\n",
    "  model.to(device)\n",
    "  optimizer = torch.optim.Adam(model.parameters(),lr=1e-3)\n",
    "  for e in range(EPOCH):\n",
    "    start = time.time()\n",
    "\n",
    "    train_loss = train(model,trainloader,optimizer)\n",
    "    test_loss = evaluation(model,testloader)\n",
    "\n",
    "    train_loss_vec.append(train_loss)\n",
    "    test_loss_vec.append(test_loss)\n",
    "\n",
    "    end = time.time()\n",
    "    elapsed = end-start\n",
    "    print(\"At validation {0:d}, iteration {1:d}, the train loss is {2:.3f} and test loss is {3:.3f},time it takes is {4:.3f}\".format(num_val,e+1,train_loss,test_loss,elapsed))\n",
    "  if test_loss < best_val_loss:\n",
    "    best_val_loss = test_loss\n",
    "    torch.save(model.state_dict(), \"Transformer_{}.pt\".format(num_val))\n",
    "\n",
    "    best_train_loss = train_loss_vec\n",
    "    best_test_loss = test_loss_vec\n",
    "  num_val += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "VlUXLl7WaC8a",
    "outputId": "5d85c70a-b320-4328-e343-49692322c130"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = self_attn(INPUT_LANG,OUTPUT_LANG,N_LAYERS,NUM_HEAD,EMBEDDED_SIZE,HIDDEN_SIZE,device,DROP_P).to(device)\n",
    "model.load_state_dict(torch.load(\"Transformer_6.pt\",map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oDel7Q6d4i_B"
   },
   "source": [
    "# **BLEU evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jRj0bIq1aKxX"
   },
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "def bleu_score(ref,can,n_gram=4):    \n",
    "    if n_gram==4:    # 4-gram\n",
    "        weights=(0.25,0.25,0.25,0.25)\n",
    "    elif n_gram==3:  # 3-gram\n",
    "        weights=(0.33,0.33,0.33,0)\n",
    "    elif n_gram==2:  # 2-gram\n",
    "        weights=(0.5,0.5,0,0)        \n",
    "    elif n_gram==1:  # 1-gram\n",
    "        weights=(1,0,0,0)    \n",
    "    else:\n",
    "        print(\"wrong n_gram\")\n",
    "        return 0\n",
    "    return sentence_bleu(ref, can, weights)*100\n",
    "\n",
    "def ref_transform(ref):    \n",
    "    ref=re.sub(r\"([，；:。？、！])\",r\" \",ref)\n",
    "    ref=ref.split()\n",
    "\n",
    "    n_blank=0\n",
    "    for i in range(len(ref)):\n",
    "        if ref[i]=='':\n",
    "            n_blank=n_blank+1\n",
    "    for i in range(n_blank):\n",
    "        ref.remove('')\n",
    "    return ref\n",
    "\n",
    "def can_transform(result):\n",
    "    result=re.sub(r\"([，；:。？、！])\",r\" \",result)\n",
    "    can=[]\n",
    "    for i in range(len(result)):\n",
    "        if result[i]!=' ':\n",
    "            can.append(result[i])\n",
    "    return can"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "colab_type": "code",
    "id": "vPE_wVsk4o8g",
    "outputId": "7e933ce8-9c8c-4f1d-b1c1-2f967c6ab64a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-178-a6e938e86ee5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpairs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mref_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mresult\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpairs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mcan\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcan_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mref_can_transformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcan\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-171-124e8e0a99ae>\u001b[0m in \u001b[0;36mtranslate\u001b[0;34m(model, input_, device, maxIter)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0;31m# trg_output = [batch_size, trg_len, output_size]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m             \u001b[0mtrg_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrg_att\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrg_tensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msrc_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrg_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msrc_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0mattention\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrg_att\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-86-cf68065c49da>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_, encoder_input, trg_mask, src_mask)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mattention\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencoder_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrg_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msrc_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;31m# x is of shape (1,trg_len,output)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-84-8b7faaafce6c>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, trg, src, trg_mask, src_mask)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;31m# encoder_decoder attention layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m# attention is of shape (batch_size,num_head,trg_len,src_Len)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0moutput2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mattention\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msrc_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;31m# second layer normalization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-73-dcd5c81f2d1d>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, query, key, value, mask)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;31m#score has shape (batch_size, num_heads,query_len,key_len)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ref_can_transformer=[]\n",
    "for i in range(len(pairs)):\n",
    "    ref=pairs[i][1]\n",
    "    ref=ref_transform(ref)\n",
    "    result=translate(model,pairs[i][0],device)[0]\n",
    "    can=can_transform(result)\n",
    "    ref_can_transformer.append([ref,can])\n",
    "    if i%999==0:\n",
    "      print(i)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Transformer",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
